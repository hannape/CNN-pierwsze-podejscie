{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "birddet_baseline3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hannape/CNN-pierwsze-podejscie/blob/master/birddet_baseline3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "AKW50h8JPURM",
        "colab_type": "code",
        "outputId": "413dc41a-5e2a-43f8-ad90-8eb3e6bba4da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# DCASE 2018 - Bird Audio Detection challenge (Task 3)\n",
        "\n",
        "# This code is a basic implementation of bird audio detector (based on baseline code's architecture)\n",
        "%cd '/content/drive/My Drive/ukybirddet'\n",
        "import h5py\n",
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import PIL.Image\n",
        "import matplotlib.pyplot as plt\n",
        "from HTK import HTKFile\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "\n",
        "import keras\n",
        "from keras.layers import Conv2D, Dropout, MaxPooling2D, Dense, GlobalAveragePooling2D, Flatten, BatchNormalization, AveragePooling2D\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.losses import binary_crossentropy, mean_squared_error, mean_absolute_error\n",
        "from keras.regularizers import l2\n",
        "\n",
        "import my_callbacks\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import CSVLogger\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ukybirddet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k7hJPV84Pdks",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################################################\n",
        "#\n",
        "#   Global parameters\n",
        "#\n",
        "################################################\n",
        "\n",
        "#checking mfc features\n",
        "SPECTPATH = 'workingfiles/features_baseline/'\n",
        "LABELPATH = 'labels/'\n",
        "FILELIST = 'workingfiles/filelists/'\n",
        "\n",
        "RESULTPATH = 'trained_model/baseline/'\n",
        "SUBMISSIONFILE = 'DCASE_submission_baseline.csv'\n",
        "PREDICTIONPATH = 'prediction/'\n",
        "dataset = ['BirdVox-DCASE-20k.csv', 'ff1010bird.csv', 'warblrb10k.csv']\n",
        "\n",
        "logfile_name = RESULTPATH + '40ep_logfile.log'\n",
        "checkpoint_model_name = RESULTPATH + '40ep_ckpt.h5'\n",
        "final_model_name = RESULTPATH + '_60ep_flmdl.h5'\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "EPOCH_SIZE = 20   ## CHANGED HANIA\n",
        "AUGMENT_SIZE = 1\n",
        "with_augmentation = False\n",
        "domain_adaptation = False\n",
        "features='h5'\n",
        "model_operation = 'new'\n",
        "# model_operations : 'new', 'load', 'test'\n",
        "shape = (700, 80)\n",
        "expected_shape = (700, 80)\n",
        "spect = np.zeros(shape)\n",
        "label = np.zeros(1)\n",
        "transform_for_birdvox = np.zeros((80,80))\n",
        "transform_for_ff1010bird = np.zeros((80,80))\n",
        "transform_for_chern = np.zeros((80,80))\n",
        "transform_for_poland = np.zeros((80,80))\n",
        "\n",
        "# Callbacks for logging during epochs\n",
        "reduceLR = ReduceLROnPlateau(factor=0.2, patience=5, min_lr=0.00001)\n",
        "checkPoint = ModelCheckpoint(filepath = checkpoint_model_name, monitor= 'val_acc', mode = 'max', save_best_only=True)\n",
        "csvLogger = CSVLogger(logfile_name, separator=',', append=False)\n",
        "\n",
        "################################################\n",
        "#\n",
        "#   Data set selection\n",
        "#\n",
        "################################################\n",
        "\n",
        "# Parameters in this section can be adjusted to select different data sets to train, test, and validate on.\n",
        "\n",
        "# Keys by which we will access properties of a data set. The values assigned here are ultimately meaningless.\n",
        "# The 'k' prefix on these declarations signify that they will be used as keys in a dictionary.\n",
        "k_VAL_FILE = 'validation_file_path'\n",
        "k_TEST_FILE = 'test_file_path'\n",
        "k_TRAIN_FILE = 'train_file_path'\n",
        "k_VAL_SIZE = 'validate_size'\n",
        "k_TEST_SIZE = 'test_size'\n",
        "k_TRAIN_SIZE = 'train_size'\n",
        "k_CLASS_WEIGHT = 'class_weight'\n",
        "#k_TRANSFORM_MATRIX = 'transform_matrix'\n",
        "f_TRANSFORM_SRC_BIRDVOX= 'adaptation_files/transform_source_700_BirdVox-DCASE-20k.h5'\n",
        "f_TRANSFORM_SRC_FF1010BIRD='adaptation_files/transform_source_700_ff1010bird.h5'\n",
        "f_TRANSFORM_SRC_POLANDNFC='adaptation_files/transform_source_700_PolandNFC.h5'\n",
        "f_TRANSFORM_SRC_CHERNOBYL='adaptation_files/transform_source_700_Chernobyl.h5'\n",
        "\n",
        "# Declare the dictionaries to represent the data sets\n",
        "d_birdVox = {k_VAL_FILE: 'val_B', k_TEST_FILE: 'test_B', k_TRAIN_FILE: 'train_B',\n",
        "             k_VAL_SIZE: 1000.0, k_TEST_SIZE: 3000.0, k_TRAIN_SIZE: 16000.0,\n",
        "             k_CLASS_WEIGHT: {0: 0.50,1: 0.50}}\n",
        "d_warblr = {k_VAL_FILE: 'val_W', k_TEST_FILE: 'test_W', k_TRAIN_FILE: 'train_W',\n",
        "            k_VAL_SIZE: 400.0, k_TEST_SIZE: 1200.0, k_TRAIN_SIZE: 6400.0,\n",
        "            k_CLASS_WEIGHT: {0: 0.75, 1: 0.25}}\n",
        "d_freefield = {k_VAL_FILE: 'val_F', k_TEST_FILE: 'test_F', k_TRAIN_FILE: 'train_F',\n",
        "               k_VAL_SIZE: 385.0, k_TEST_SIZE: 1153.0, k_TRAIN_SIZE: 6152.0,\n",
        "               k_CLASS_WEIGHT: {0: 0.25, 1: 0.75}}\n",
        "d_fold1 = {k_VAL_FILE: 'test_BF', k_TEST_FILE: 'val_1', k_TRAIN_FILE: 'train_BF',\n",
        "           k_VAL_SIZE: 4153.0, k_TEST_SIZE: 8000.0, k_TRAIN_SIZE: 22152.0,\n",
        "           k_CLASS_WEIGHT: {0: 0.43, 1: 0.57}}\n",
        "d_fold2 = {k_VAL_FILE: 'test_WF', k_TEST_FILE: 'val_2', k_TRAIN_FILE: 'train_WF',\n",
        "           k_VAL_SIZE: 2353.0, k_TEST_SIZE: 20000.0, k_TRAIN_SIZE: 12552.0,\n",
        "           k_CLASS_WEIGHT: {0: 0.50, 1: 0.50}}\n",
        "d_fold3 = {k_VAL_FILE: 'test_BW', k_TEST_FILE: 'val_3', k_TRAIN_FILE: 'train_BW',\n",
        "           k_VAL_SIZE: 4200.0, k_TEST_SIZE: 7690.0, k_TRAIN_SIZE: 22400.0,\n",
        "           k_CLASS_WEIGHT: {0: 0.57, 1: 0.43}}\n",
        "d_all3 = {k_VAL_FILE: 'val_BWF', k_TEST_FILE:'test', k_TRAIN_FILE: 'train_BWF',\n",
        "           k_VAL_SIZE: 1785.0, k_TEST_SIZE: 12620.0, k_TRAIN_SIZE: 35960.0,\n",
        "           k_CLASS_WEIGHT: {0: 0.50, 1: 0.50}}\n",
        "d_moj = {k_VAL_FILE: 'val_BWF_moj', k_TEST_FILE:'test_moj_NFC', k_TRAIN_FILE: 'train_BWF_moj',\n",
        "           k_VAL_SIZE: 1785.0, k_TEST_SIZE: 105.0, k_TRAIN_SIZE: 35960.0,\n",
        "           k_CLASS_WEIGHT: {0: 0.50, 1: 0.50}}\n",
        "# Declare the training, validation, and testing sets here using the dictionaries defined above.\n",
        "# Set these variables to change the data set.\n",
        "training_set = d_all3\n",
        "validation_set = d_all3\n",
        "test_set = d_moj #d_all3\n",
        "\n",
        "# Grab the file lists and sizes from the corresponding data sets.\n",
        "train_filelist = FILELIST + training_set[k_TRAIN_FILE]\n",
        "TRAIN_SIZE = training_set[k_TRAIN_SIZE]\n",
        "\n",
        "val_filelist = FILELIST + validation_set[k_VAL_FILE]\n",
        "VAL_SIZE = validation_set[k_VAL_SIZE]\n",
        "\n",
        "test_filelist = FILELIST + test_set[k_TEST_FILE]\n",
        "TEST_SIZE = test_set[k_TEST_SIZE]\n",
        "\n",
        "################################################\n",
        "#\n",
        "#   Generator with Augmentation\n",
        "#\n",
        "################################################\n",
        "\n",
        "# use this generator when augmentation is needed\n",
        "def data_generator(filelistpath, batch_size=16, shuffle=False):\n",
        "    batch_index = 0\n",
        "    image_index = -1\n",
        "    filelist = open(filelistpath, 'r')\n",
        "    filenames = filelist.readlines()\n",
        "    filelist.close()\n",
        "\n",
        "    # shuffling filelist\n",
        "    if shuffle==True:\n",
        "        random.shuffle(filenames)\n",
        "\n",
        "    dataset = ['BirdVox-DCASE-20k.csv', 'ff1010bird.csv', 'warblrb10k.csv']\n",
        "\n",
        "    labels_dict = {}\n",
        "    for n in range(len(dataset)):\n",
        "        labels_list = csv.reader(open(LABELPATH + dataset[n], 'r'))\n",
        "        next(labels_list)\n",
        "        for k, r, v in labels_list:\n",
        "            labels_dict[r + '/' + k + '.wav'] = v\n",
        "\n",
        "    while True:\n",
        "        image_index = (image_index + 1) % len(filenames)\n",
        "\n",
        "        # if shuffle and image_index = 0\n",
        "        # shuffling filelist\n",
        "        if shuffle == True and image_index == 0:\n",
        "            random.shuffle(filenames)\n",
        "\n",
        "        file_id = filenames[image_index].rstrip()\n",
        "\n",
        "        if batch_index == 0:\n",
        "            # re-initialize spectrogram and label batch\n",
        "            spect_batch = np.zeros([1, spect.shape[0], spect.shape[1], 1])\n",
        "            label_batch = np.zeros([1, 1])\n",
        "            aug_spect_batch = np.zeros([batch_size, spect.shape[0], spect.shape[1], 1])\n",
        "            aug_label_batch = np.zeros([batch_size, 1])\n",
        "\n",
        "        if features=='h5':\n",
        "            hf = h5py.File(SPECTPATH + file_id + '.h5', 'r')\n",
        "            imagedata = hf.get('features')\n",
        "            imagedata = np.array(imagedata)\n",
        "            hf.close()\n",
        "            # normalizing intensity values of spectrogram from [-15.0966 to 2.25745] to [0 to 1] range\n",
        "            imagedata = (imagedata + 15.0966)/(15.0966 + 2.25745)\n",
        "        elif features == 'mfc':\n",
        "            htk_reader = HTKFile()\n",
        "            htk_reader.load(SPECTPATH + file_id[:-4] + '.mfc')\n",
        "            imagedata = np.array(htk_reader.data)\n",
        "            imagedata = imagedata / 17.0\n",
        "\n",
        "        # processing files with shapes other than expected shape in warblr dataset\n",
        "\n",
        "        if imagedata.shape[0] != expected_shape[0]:\n",
        "            old_imagedata = imagedata\n",
        "            imagedata = np.zeros(expected_shape)\n",
        "\n",
        "            if old_imagedata.shape[0] < expected_shape[0]:\n",
        "\n",
        "                diff_in_frames = expected_shape[0] - old_imagedata.shape[0]\n",
        "                if diff_in_frames < expected_shape[0] / 2:\n",
        "                    imagedata = np.vstack((old_imagedata, old_imagedata[\n",
        "                        range(old_imagedata.shape[0] - diff_in_frames, old_imagedata.shape[0])]))\n",
        "\n",
        "                elif diff_in_frames > expected_shape[0] / 2:\n",
        "                    count = np.floor(expected_shape[0] / old_imagedata.shape[0])\n",
        "                    remaining_diff = (expected_shape[0] - old_imagedata.shape[0] * int(count))\n",
        "                    imagedata = np.vstack(([old_imagedata] * int(count)))\n",
        "                    imagedata = np.vstack(\n",
        "                        (imagedata, old_imagedata[range(old_imagedata.shape[0] - remaining_diff, old_imagedata.shape[0])]))\n",
        "\n",
        "            elif old_imagedata.shape[0] > expected_shape[0]:\n",
        "                diff_in_frames = old_imagedata.shape[0] - expected_shape[0]\n",
        "\n",
        "                if diff_in_frames < expected_shape[0] / 2:\n",
        "                    imagedata[range(0, diff_in_frames + 1), :] = np.mean(np.array([old_imagedata[range(0, diff_in_frames + 1), :],old_imagedata[range(old_imagedata.shape[0] - diff_in_frames - 1, old_imagedata.shape[0]), :]]),axis=0)\n",
        "                    imagedata[range(diff_in_frames + 1, expected_shape[0]), :] = old_imagedata[range(diff_in_frames + 1, expected_shape[0])]\n",
        "\n",
        "                elif diff_in_frames > expected_shape[0] / 2:\n",
        "                    count = int(np.floor(old_imagedata.shape[0] / expected_shape[0]))\n",
        "                    remaining_diff = (old_imagedata.shape[0] - expected_shape[0] * count)\n",
        "                    for index in range(0, count):\n",
        "                        imagedata[range(0, expected_shape[0]), :] = np.sum([imagedata, old_imagedata[range(index * expected_shape[0], (index + 1) * expected_shape[0])]],axis=0) / count\n",
        "                        imagedata[range(0, remaining_diff), :] = np.mean(np.array([old_imagedata[range(old_imagedata.shape[0] - remaining_diff, old_imagedata.shape[0]), :],imagedata[range(0, remaining_diff), :]]), axis=0)\n",
        "\n",
        "\n",
        "        imagedata = np.reshape(imagedata, (1, imagedata.shape[0], imagedata.shape[1], 1))\n",
        "\n",
        "        spect_batch[0, :, :, :] = imagedata\n",
        "        label_batch[0, :] = labels_dict[file_id]\n",
        "\n",
        "        gen_img = datagen.flow(imagedata, label_batch[0, :], batch_size=1, shuffle=False, save_to_dir=None)\n",
        "        aug_spect_batch[batch_index, :, :, :] = imagedata\n",
        "        aug_label_batch[batch_index, :] = label_batch[0, :]\n",
        "        batch_index += 1\n",
        "\n",
        "        for n in range(AUGMENT_SIZE-1):\n",
        "            aug_spect_batch[batch_index, :, :, :], aug_label_batch[batch_index, :] = gen_img.next()\n",
        "            batch_index += 1\n",
        "            if batch_index >= batch_size:\n",
        "                batch_index = 0\n",
        "                inputs = [aug_spect_batch]\n",
        "                outputs = [aug_label_batch]\n",
        "                yield inputs, outputs\n",
        "\n",
        "\n",
        "################################################\n",
        "#\n",
        "#   Generator without Augmentation\n",
        "#\n",
        "################################################\n",
        "\n",
        "def dataval_generator(filelistpath, batch_size=32, shuffle=False):\n",
        "    batch_index = 0\n",
        "    image_index = -1\n",
        "\n",
        "    filelist = open(filelistpath, 'r')\n",
        "    filenames = filelist.readlines()\n",
        "    filelist.close()\n",
        "\n",
        "    dataset = (['PolandNFC.csv'])#dataset = (['Chernobyl.csv', 'PolandNFC.csv', 'warblrb10k-eval.csv'])\n",
        "\n",
        "    labels_dict = {}\n",
        "    for n in range(len(dataset)):\n",
        "        labels_list = csv.reader(open(LABELPATH + dataset[n], 'r'))\n",
        "        next(labels_list)\n",
        "        for k, r, v in labels_list:\n",
        "            labels_dict[r + '/' + k + '.wav'] = v\n",
        "\n",
        "    while True:\n",
        "        image_index = (image_index + 1) % len(filenames)\n",
        "\n",
        "        # if shuffle and image_index = 0\n",
        "        # shuffling filelist\n",
        "        if shuffle == True and image_index == 0:\n",
        "            random.shuffle(filenames)\n",
        "\n",
        "        file_id = filenames[image_index].rstrip()\n",
        "\n",
        "        if batch_index == 0:\n",
        "            # re-initialize spectrogram and label batch\n",
        "            spect_batch = np.zeros([batch_size, spect.shape[0], spect.shape[1], 1])\n",
        "            label_batch = np.zeros([batch_size, 1])\n",
        "\n",
        "        if features == 'h5':\n",
        "            #file_prefix = file_id[:file_id.rfind(\"/\")+1]\n",
        "            #file_suffix = file_id[file_id.rfind(\"/\")+1:]\n",
        "            #hf = h5py.File(SPECTPATH + file_prefix + 'enhanced_'+ file_suffix + '.h5')\n",
        "            hf = h5py.File(SPECTPATH + file_id + '.h5', 'r')#[:-4]for evaluation dataset\n",
        "            imagedata = hf.get('features')\n",
        "            imagedata = np.array(imagedata)\n",
        "            hf.close()\n",
        "\n",
        "            # normalizing intensity values of spectrogram from [-15.0966 to 2.25745] to [0 to 1] range\n",
        "            imagedata = (imagedata + 15.0966)/(15.0966 + 2.25745)\n",
        "\n",
        "        elif features == 'mfc':\n",
        "            htk_reader = HTKFile()\n",
        "            #file_prefix = file_id[:file_id.rfind(\"/\")+1]\n",
        "            #file_suffix = file_id[file_id.rfind(\"/\")+1:]\n",
        "            #htk_reader.load(SPECTPATH + file_prefix + 'enhanced_'+ file_suffix[:-4] + '.mfc')\n",
        "            htk_reader.load(SPECTPATH + file_id[:-4] + '.mfc')\n",
        "            imagedata = np.array(htk_reader.data)\n",
        "            imagedata = imagedata/17.0\n",
        "\n",
        "        # processing files with shapes other than expected shape in warblr dataset\n",
        "\n",
        "        if imagedata.shape[0] != expected_shape[0]:\n",
        "            old_imagedata = imagedata\n",
        "            imagedata = np.zeros(expected_shape)\n",
        "\n",
        "            if old_imagedata.shape[0] < expected_shape[0]:\n",
        "\n",
        "                diff_in_frames = expected_shape[0] - old_imagedata.shape[0]\n",
        "                if diff_in_frames < expected_shape[0] / 2:\n",
        "                    imagedata = np.vstack((old_imagedata, old_imagedata[\n",
        "                        range(old_imagedata.shape[0] - diff_in_frames, old_imagedata.shape[0])]))\n",
        "\n",
        "                elif diff_in_frames > expected_shape[0] / 2:\n",
        "                    count = np.floor(expected_shape[0] / old_imagedata.shape[0])\n",
        "                    remaining_diff = (expected_shape[0] - old_imagedata.shape[0] * int(count))\n",
        "                    imagedata = np.vstack(([old_imagedata] * int(count)))\n",
        "                    imagedata = np.vstack(\n",
        "                        (imagedata, old_imagedata[range(old_imagedata.shape[0] - remaining_diff, old_imagedata.shape[0])]))\n",
        "\n",
        "            elif old_imagedata.shape[0] > expected_shape[0]:\n",
        "                diff_in_frames = old_imagedata.shape[0] - expected_shape[0]\n",
        "\n",
        "                if diff_in_frames < expected_shape[0] / 2:\n",
        "                    imagedata[range(0, diff_in_frames + 1), :] = np.mean(np.array([old_imagedata[range(0, diff_in_frames + 1), :],old_imagedata[range(old_imagedata.shape[0] - diff_in_frames - 1, old_imagedata.shape[0]), :]]),axis=0)\n",
        "                    imagedata[range(diff_in_frames + 1, expected_shape[0]), :] = old_imagedata[range(diff_in_frames + 1, expected_shape[0])]\n",
        "\n",
        "                elif diff_in_frames > expected_shape[0] / 2:\n",
        "                    count = int(np.floor(old_imagedata.shape[0] / expected_shape[0]))\n",
        "                    remaining_diff = (old_imagedata.shape[0] - expected_shape[0] * count)\n",
        "                    for index in range(0, count):\n",
        "                        imagedata[range(0, expected_shape[0]), :] = np.sum([imagedata, old_imagedata[range(index * expected_shape[0], (index + 1) * expected_shape[0])]],axis=0) / count\n",
        "                        imagedata[range(0, remaining_diff), :] = np.mean(np.array([old_imagedata[range(old_imagedata.shape[0] - remaining_diff, old_imagedata.shape[0]), :],imagedata[range(0, remaining_diff), :]]), axis=0)\n",
        "\n",
        "        if domain_adaptation == True:\n",
        "            filedataset = file_id[:file_id.rfind('/')]\n",
        "            #print('Domain adaptation is supposed to be off')\n",
        "            if filedataset == 'BirdVox-DCASE-20k':\n",
        "                imagedata = np.matmul(imagedata, transform_for_birdvox)\n",
        "                imagedata = (imagedata - 3.4) / (6.95 - 3.4)\n",
        "                #min: 3.4020782 - -max:6.9419036\n",
        "\n",
        "            elif filedataset == 'ff1010bird':\n",
        "                imagedata = np.matmul(imagedata, transform_for_ff1010bird)\n",
        "                imagedata = (imagedata - 1.4) / (7.37 - 1.4)\n",
        "                # min:1.4374458--max:7.363845\n",
        "\n",
        "            elif filedataset == 'Chernobyl':\n",
        "                imagedata = np.matmul(imagedata, transform_for_chern)\n",
        "                imagedata = (imagedata - 3.75) / (7 - 3.75)\n",
        "                #3.7511292--max:7.00125\n",
        "\n",
        "            elif filedataset == 'PolandNFC':\n",
        "                imagedata = np.matmul(imagedata, transform_for_poland)\n",
        "                imagedata = (imagedata + 10.8) / (10.8 + 7.40)\n",
        "                # -10.796116--max:7.4045897\n",
        "        imagedata = np.reshape(imagedata, (1, imagedata.shape[0], imagedata.shape[1], 1))\n",
        "\n",
        "        spect_batch[batch_index, :, :, :] = imagedata\n",
        "        if model_operation != 'test':\n",
        "            label_batch[batch_index, :] = labels_dict[file_id]\n",
        "\n",
        "        batch_index += 1\n",
        "\n",
        "        if batch_index >= batch_size:\n",
        "            batch_index = 0\n",
        "            inputs = [spect_batch]\n",
        "            outputs = [label_batch]\n",
        "            yield inputs, outputs\n",
        "\n",
        "def datatest_generator(filelistpath, batch_size=32, shuffle=False):\n",
        "    batch_index = 0\n",
        "    image_index = -1\n",
        "\n",
        "    filelist = open(filelistpath, 'r')\n",
        "    filenames = filelist.readlines()\n",
        "    filelist.close()\n",
        "\n",
        "    dataset = (['PolandNEW.csv'])#(['Chernobyl.csv', 'PolandNFC.csv', 'warblrb10k-eval.csv'])\n",
        "\n",
        "    labels_dict = {}\n",
        "    for n in range(len(dataset)):\n",
        "        labels_list = csv.reader(open(LABELPATH + dataset[n], 'r'))\n",
        "        next(labels_list)\n",
        "        for k, r, v in labels_list:\n",
        "            labels_dict[r + '/' + k] = v\n",
        "\n",
        "    while True:\n",
        "        image_index = (image_index + 1) % len(filenames)\n",
        "\n",
        "        # if shuffle and image_index = 0\n",
        "        # shuffling filelist\n",
        "        if shuffle == True and image_index == 0:\n",
        "            random.shuffle(filenames)\n",
        "\n",
        "        file_id = filenames[image_index].rstrip()\n",
        "\n",
        "        if batch_index == 0:\n",
        "            # re-initialize spectrogram and label batch\n",
        "            spect_batch = np.zeros([batch_size, spect.shape[0], spect.shape[1], 1])\n",
        "            label_batch = np.zeros([batch_size, 1])\n",
        "\n",
        "        if features == 'h5':\n",
        "            #file_prefix = file_id[:file_id.rfind(\"/\")+1]\n",
        "            #file_suffix = file_id[file_id.rfind(\"/\")+1:]\n",
        "            #hf = h5py.File(SPECTPATH + file_prefix + 'enhanced_'+ file_suffix + '.h5')\n",
        "            hf = h5py.File(SPECTPATH + file_id[:-4] + '.h5', 'r')#[:-4]for evaluation dataset\n",
        "            imagedata = hf.get('features')\n",
        "            imagedata = np.array(imagedata)\n",
        "            hf.close()\n",
        "\n",
        "            # normalizing intensity values of spectrogram from [-15.0966 to 2.25745] to [0 to 1] range\n",
        "            imagedata = (imagedata + 15.0966)/(15.0966 + 2.25745)\n",
        "\n",
        "        elif features == 'mfc':\n",
        "            htk_reader = HTKFile()\n",
        "            #file_prefix = file_id[:file_id.rfind(\"/\")+1]\n",
        "            #file_suffix = file_id[file_id.rfind(\"/\")+1:]\n",
        "            #htk_reader.load(SPECTPATH + file_prefix + 'enhanced_'+ file_suffix[:-4] + '.mfc')\n",
        "            htk_reader.load(SPECTPATH + file_id[:-8] + '.mfc')\n",
        "            imagedata = np.array(htk_reader.data)\n",
        "            imagedata = imagedata/17.0\n",
        "\n",
        "        # processing files with shapes other than expected shape in warblr dataset\n",
        "\n",
        "        if imagedata.shape[0] != expected_shape[0]:\n",
        "            old_imagedata = imagedata\n",
        "            imagedata = np.zeros(expected_shape)\n",
        "\n",
        "            if old_imagedata.shape[0] < expected_shape[0]:\n",
        "\n",
        "                diff_in_frames = expected_shape[0] - old_imagedata.shape[0]\n",
        "                if diff_in_frames < expected_shape[0] / 2:\n",
        "                    imagedata = np.vstack((old_imagedata, old_imagedata[\n",
        "                        range(old_imagedata.shape[0] - diff_in_frames, old_imagedata.shape[0])]))\n",
        "\n",
        "                elif diff_in_frames > expected_shape[0] / 2:\n",
        "                    count = np.floor(expected_shape[0] / old_imagedata.shape[0])\n",
        "                    remaining_diff = (expected_shape[0] - old_imagedata.shape[0] * int(count))\n",
        "                    imagedata = np.vstack(([old_imagedata] * int(count)))\n",
        "                    imagedata = np.vstack(\n",
        "                        (imagedata, old_imagedata[range(old_imagedata.shape[0] - remaining_diff, old_imagedata.shape[0])]))\n",
        "\n",
        "            elif old_imagedata.shape[0] > expected_shape[0]:\n",
        "                diff_in_frames = old_imagedata.shape[0] - expected_shape[0]\n",
        "\n",
        "                if diff_in_frames < expected_shape[0] / 2:\n",
        "                    imagedata[range(0, diff_in_frames + 1), :] = np.mean(np.array([old_imagedata[range(0, diff_in_frames + 1), :],old_imagedata[range(old_imagedata.shape[0] - diff_in_frames - 1, old_imagedata.shape[0]), :]]),axis=0)\n",
        "                    imagedata[range(diff_in_frames + 1, expected_shape[0]), :] = old_imagedata[range(diff_in_frames + 1, expected_shape[0])]\n",
        "\n",
        "                elif diff_in_frames > expected_shape[0] / 2:\n",
        "                    count = int(np.floor(old_imagedata.shape[0] / expected_shape[0]))\n",
        "                    remaining_diff = (old_imagedata.shape[0] - expected_shape[0] * count)\n",
        "                    for index in range(0, count):\n",
        "                        imagedata[range(0, expected_shape[0]), :] = np.sum([imagedata, old_imagedata[range(index * expected_shape[0], (index + 1) * expected_shape[0])]],axis=0) / count\n",
        "                        imagedata[range(0, remaining_diff), :] = np.mean(np.array([old_imagedata[range(old_imagedata.shape[0] - remaining_diff, old_imagedata.shape[0]), :],imagedata[range(0, remaining_diff), :]]), axis=0)\n",
        "\n",
        "        if domain_adaptation == True:\n",
        "            filedataset = file_id[:file_id.rfind('/')]\n",
        "            #print('Domain adaptation is supposed to be off')\n",
        "            if filedataset == 'BirdVox-DCASE-20k':\n",
        "                imagedata = np.matmul(imagedata, transform_for_birdvox)\n",
        "                imagedata = (imagedata - 3.4) / (6.95 - 3.4)\n",
        "                #min: 3.4020782 - -max:6.9419036\n",
        "\n",
        "            elif filedataset == 'ff1010bird':\n",
        "                imagedata = np.matmul(imagedata, transform_for_ff1010bird)\n",
        "                imagedata = (imagedata - 1.4) / (7.37 - 1.4)\n",
        "                # min:1.4374458--max:7.363845\n",
        "\n",
        "            elif filedataset == 'Chernobyl':\n",
        "                imagedata = np.matmul(imagedata, transform_for_chern)\n",
        "                imagedata = (imagedata - 3.75) / (7 - 3.75)\n",
        "                #3.7511292--max:7.00125\n",
        "\n",
        "            elif filedataset == 'PolandNFC':\n",
        "                imagedata = np.matmul(imagedata, transform_for_poland)\n",
        "                imagedata = (imagedata + 10.8) / (10.8 + 7.40)\n",
        "                # -10.796116--max:7.4045897\n",
        "        imagedata = np.reshape(imagedata, (1, imagedata.shape[0], imagedata.shape[1], 1))\n",
        "\n",
        "        spect_batch[batch_index, :, :, :] = imagedata\n",
        "\n",
        "        batch_index += 1\n",
        "\n",
        "        if batch_index >= batch_size:\n",
        "            batch_index = 0\n",
        "            inputs = [spect_batch]\n",
        "            yield inputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZOvew26XfNOu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################################################\n",
        "#\n",
        "#   ROC Label Generation\n",
        "#\n",
        "################################################\n",
        "\n",
        "def testdata(filelistpath, test_size):\n",
        "    image_index = -1\n",
        "\n",
        "    filelist = open(filelistpath, 'r')\n",
        "    filenames = filelist.readlines()\n",
        "    filelist.close()\n",
        "\n",
        "    #dataset = (['Chernobyl.csv', 'PolandNFC.csv', 'warblrb10k-eval.csv'])\n",
        "\n",
        "    labels_dict = {}\n",
        "    for n in range(len(dataset)):\n",
        "        labels_list = csv.reader(open(LABELPATH + dataset[n], 'r'))\n",
        "        next(labels_list)\n",
        "        for k, r, v in labels_list:\n",
        "            labels_dict[r + '/' + k + '.wav'] = v\n",
        "\n",
        "    label_batch = np.zeros([int(test_size), 1])\n",
        "\n",
        "    for m in range(len(filenames)):\n",
        "        image_index = (image_index + 1) % len(filenames)\n",
        "\n",
        "        file_id = filenames[image_index].rstrip()\n",
        "\n",
        "        label_batch[image_index, :] = labels_dict[file_id]\n",
        "\n",
        "        outputs = [label_batch]\n",
        "\n",
        "    return outputs\n",
        "\n",
        "################################################\n",
        "#\n",
        "#   Reading covariance transforms\n",
        "#\n",
        "################################################\n",
        "\n",
        "#reading birdvox transform\n",
        "htf = h5py.File(f_TRANSFORM_SRC_BIRDVOX, 'r')\n",
        "transform_for_birdvox = htf.get('cov')\n",
        "transform_for_birdvox = np.array(transform_for_birdvox)\n",
        "htf.close()\n",
        "\n",
        "#reading ff1010bird transform\n",
        "htf = h5py.File(f_TRANSFORM_SRC_FF1010BIRD, 'r')\n",
        "transform_for_ff1010bird = htf.get('cov')\n",
        "transform_for_ff1010bird = np.array(transform_for_ff1010bird)\n",
        "htf.close()\n",
        "\n",
        "#reading chernobyl transform\n",
        "htf = h5py.File(f_TRANSFORM_SRC_CHERNOBYL, 'r')\n",
        "transform_for_chern = htf.get('cov')\n",
        "transform_for_chern = np.array(transform_for_chern)\n",
        "htf.close()\n",
        "\n",
        "#reading polandnfc transform\n",
        "htf = h5py.File(f_TRANSFORM_SRC_POLANDNFC, 'r')\n",
        "transform_for_poland = htf.get('cov')\n",
        "transform_for_poland = np.array(transform_for_poland)\n",
        "htf.close()\n",
        "\n",
        "#reading polandnew transform\n",
        "htf = h5py.File(f_TRANSFORM_SRC_POLANDNEW, 'r')\n",
        "transform_for_poland_new = htf.get('cov')\n",
        "transform_for_poland_new = np.array(transform_for_poland_new)\n",
        "htf.close()\n",
        "\n",
        "\n",
        "if(with_augmentation == True):\n",
        "    train_generator = data_generator(train_filelist, BATCH_SIZE, True)\n",
        "else:\n",
        "    train_generator = dataval_generator(train_filelist, BATCH_SIZE, True)\n",
        "\n",
        "validation_generator = dataval_generator(val_filelist, BATCH_SIZE, False)\n",
        "test_generator = datatest_generator(test_filelist, BATCH_SIZE, False)\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,\n",
        "    featurewise_std_normalization=False,\n",
        "    rotation_range=0,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.9,\n",
        "    horizontal_flip=False,\n",
        "    fill_mode=\"wrap\")\n",
        "\n",
        "################################################\n",
        "#\n",
        "#   Model Creation\n",
        "#\n",
        "################################################\n",
        "if model_operation == 'new':\n",
        "    model = Sequential()\n",
        "    # augmentation generator\n",
        "    # code from baseline : \"augment:Rotation|augment:Shift(low=-1,high=1,axis=3)\"\n",
        "    # keras augmentation:\n",
        "    #preprocessing_function\n",
        "\n",
        "    # convolution layers\n",
        "    model.add(Conv2D(16, (3, 3), padding='valid', input_shape=(700, 80, 1), ))  # low: try different kernel_initializer\n",
        "    model.add(BatchNormalization())  # explore order of Batchnorm and activation\n",
        "    model.add(LeakyReLU(alpha=.001))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3)))  # experiment with using smaller pooling along frequency axis\n",
        "    model.add(Conv2D(16, (3, 3), padding='valid'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=.001))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "    model.add(Conv2D(16, (3, 3), padding='valid'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=.001))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 1)))\n",
        "    model.add(Conv2D(16, (3, 3), padding='valid', kernel_regularizer=l2(0.01)))  # drfault 0.01. Try 0.001 and 0.001\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=.001))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 1)))\n",
        "\n",
        "    # dense layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(256))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=.001))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=.001))  # leaky relu value is very small experiment with bigger ones\n",
        "    model.add(Dropout(0.5))  # experiment with removing this dropout\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "elif model_operation == 'load' or model_operation == 'test':\n",
        "    model = load_model(RESULTPATH + 'flmdl.h5')\n",
        "\n",
        "if model_operation == 'new' or model_operation == 'load':\n",
        "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
        "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "    # prepare callback\n",
        "    histories = my_callbacks.Histories()\n",
        "\n",
        "model.summary()\n",
        "\n",
        "my_steps = np.floor(TRAIN_SIZE*AUGMENT_SIZE / BATCH_SIZE)\n",
        "my_val_steps = np.floor(VAL_SIZE / BATCH_SIZE)\n",
        "my_test_steps = np.ceil(TEST_SIZE / BATCH_SIZE)\n",
        "\n",
        "if model_operation == 'new' or model_operation == 'load':\n",
        "    model = load_model(\"/content/drive/My Drive/ukybirddet/trained_model/baseline/_40ep_flmdl.h5\")                        ## HANIA  CHANGED\n",
        "    history = model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=my_steps,\n",
        "        epochs=EPOCH_SIZE,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=my_val_steps,\n",
        "        callbacks= [checkPoint, reduceLR, csvLogger],\n",
        "        class_weight= training_set[k_CLASS_WEIGHT],\n",
        "        verbose=True)\n",
        "\n",
        "    #model.save(final_model_name)  ## CHANGED HANIA\n",
        "    print('Training done. The results are in : '+RESULTPATH)\n",
        "\n",
        "# generating prediction values for computing ROC_AUC score\n",
        "# whether model_operation is 'new', 'load' or 'test'\n",
        "\n",
        "pred_generator = datatest_generator(test_filelist, BATCH_SIZE, False)\n",
        "y_pred = model.predict_generator(\n",
        "    pred_generator,\n",
        "    steps=my_test_steps)\n",
        "print(y_pred)\n",
        "\n",
        "# saving predictions in csv file\n",
        "\n",
        "testfile = open(test_filelist, 'r')\n",
        "testfilenames = testfile.readlines()\n",
        "testfile.close()\n",
        "\n",
        "fidwr = open(PREDICTIONPATH+SUBMISSIONFILE, 'wt')\n",
        "try:\n",
        "    writer = csv.writer(fidwr)\n",
        "    for i in range(len(testfilenames)):\n",
        "        strf = testfilenames[i]\n",
        "        writer.writerow((strf[strf.find('/')+1:-9], str(float(y_pred[i]))))\n",
        "finally:\n",
        "    fidwr.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "APw5I4ADPdw3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from __future__ import print_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-LBGtDdPzBTv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "091e60ff-6025-4906-ec31-7604dc049b58"
      },
      "cell_type": "code",
      "source": [
        "print(test_filelist)\n",
        "print(my_test_steps)\n",
        "print(pred_generator)"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "workingfiles/filelists/test_moj_NFC\n",
            "7.0\n",
            "<generator object datatest_generator at 0x7f02cf6990f8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pV_ufWPH0XvA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, load_model\n",
        "my_test_steps = np.ceil(TEST_SIZE / BATCH_SIZE)\n",
        "\n",
        "\n",
        "model = load_model(\"/content/drive/My Drive/ukybirddet/trained_model/baseline/_60ep_flmdl.h5\")\n",
        "pred_generator = datatest_generator(test_filelist, BATCH_SIZE, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T80FoNn4Pd6V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred = model.predict_generator(\n",
        "    pred_generator,\n",
        "    steps=my_test_steps)\n",
        "print(y_pred)\n",
        "\n",
        "\n",
        "testfile = open(test_filelist, 'r')\n",
        "testfilenames = testfile.readlines()\n",
        "testfile.close()\n",
        "\n",
        "fidwr = open(PREDICTIONPATH+SUBMISSIONFILE, 'wt')\n",
        "try:\n",
        "    writer = csv.writer(fidwr)\n",
        "    for i in range(len(testfilenames)):\n",
        "        strf = testfilenames[i]\n",
        "        writer.writerow((strf[strf.find('/')+1:-9], str(float(y_pred[i]))))\n",
        "finally:\n",
        "    fidwr.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "haPxsXmC_0E2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "outputId": "391ef288-80b4-49c7-f578-b72f8cec6169"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "def readcsv(filename):\n",
        "    data = pd.read_csv(filename) \n",
        "    return(np.array(data))\n",
        "\n",
        "y_score = readcsv('prediction/DCASE_submission_baseline.csv')\n",
        "y_true = readcsv('prediction/real_DCASE_submission_baseline.csv')\n",
        "#print(np.shape(y_true[:,1])) #print(np.shape(y_score[:,1]))\n",
        "\n",
        "y_trues = y_true[:,1].astype(float)#np.array([0, 0, 1, 1]) #aaa = y_trues.astype(int) #y_trues2 =np.array([0,1,0,0])\n",
        "print(y_trues)\n",
        "y_scores = y_score[:,1]#np.array([0.1, 0.4, 0.35, 0.8])  #y_scores2 =np.array(y_score[0:4,1])\n",
        "print((y_scores))\n",
        "\n",
        "roc_auc_score(y_trues,y_scores)\n",
        "\n"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "[0.15362879633903506 0.2269481122493744 0.2488197386264801\n",
            " 0.22526180744171145 0.17333072423934934 0.20715788006782526\n",
            " 0.20156359672546387 0.18890425562858584 0.23632657527923584\n",
            " 0.2676692605018616 0.3245249688625336 0.19484958052635196\n",
            " 0.2321419417858124 0.2738488912582397 0.22162529826164248\n",
            " 0.18769603967666626 0.20070934295654294 0.2117653489112854\n",
            " 0.25636327266693115 0.18526387214660645 0.17531031370162964\n",
            " 0.23101171851158145 0.2142125368118286 0.2579026818275452\n",
            " 0.4873192608356476 0.22367846965789795 0.22800329327583316\n",
            " 0.24089950323104856 0.1998007297515869 0.2165137231349945\n",
            " 0.18371909856796265 0.1959064900875092 0.3006617724895477\n",
            " 0.22825446724891665 0.152060866355896 0.154293954372406\n",
            " 0.2266365885734558 0.1505013406276703 0.18380138278007507\n",
            " 0.15949192643165588 0.16966059803962708 0.1816319525241852\n",
            " 0.2665038108825684 0.12963077425956726 0.22539907693862915\n",
            " 0.1854541003704071 0.14647641777992249 0.18616962432861328\n",
            " 0.2370346486568451 0.23868757486343384 0.1417236626148224\n",
            " 0.19512006640434265 0.1653279960155487 0.2274994254112244\n",
            " 0.18048244714736936 0.9603841304779052 0.11554470658302307\n",
            " 0.19311991333961487 0.4956279993057251 0.20400330424308774\n",
            " 0.21579787135124207 0.23577195405960086 0.3587549924850464\n",
            " 0.15206849575042725 0.2180800437927246 0.2154945135116577\n",
            " 0.17590057849884033 0.17917683720588684 0.18030431866645813\n",
            " 0.12781932950019834 0.10325968265533447 0.13297176361083984\n",
            " 0.13500389456748962 0.1171874701976776 0.13857775926589966\n",
            " 0.1524111032485962 0.15871736407279968 0.12787479162216187\n",
            " 0.14840301871299744 0.1379442811012268 0.11351102590560913\n",
            " 0.13937413692474365 0.1411992609500885 0.12573328614234924\n",
            " 0.15741190314292908 0.13164544105529785 0.1134723424911499\n",
            " 0.11579984426498413 0.13718217611312866 0.12208983302116395\n",
            " 0.11387360095977785 0.122726172208786 0.13294556736946106\n",
            " 0.11161607503890993 0.15431290864944458 0.11968478560447693\n",
            " 0.13669142127037048 0.14918333292007446 0.1137259304523468\n",
            " 0.1152420938014984 0.11419352889060976 0.14846494793891907\n",
            " 0.12142163515090942 0.13113439083099365]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.653125"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "metadata": {
        "id": "ZQdIffyu9TPW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}