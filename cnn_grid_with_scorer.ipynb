{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_grid_with_scorer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hannape/CNN-pierwsze-podejscie/blob/master/cnn_grid_with_scorer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jUs78Pg157U2",
        "colab_type": "code",
        "outputId": "0722fe14-fd1c-45c5-d217-8163911e5b3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# previous version, with hidden code and comments - cnn_scorer_not_working\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from matplotlib import pyplot\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import  make_scorer\n",
        "from sklearn.metrics import log_loss\n",
        "K.set_image_dim_ordering('th')\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import functools\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(667)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(667)\n",
        "import random\n",
        "random.seed()\n",
        "\n",
        "X_train = numpy.load('drive/My Drive/X_train.npy')\n",
        "y_train = numpy.load('drive/My Drive/y_train.npy')\n",
        "X_test = numpy.load('drive/My Drive/X_test.npy')\n",
        "y_test = numpy.load('drive/My Drive/y_test.npy')\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, 60, 87).astype('float32')\n",
        "#X_val = X_val.reshape(X_val.shape[0], 1, 60, 87).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, 60, 87).astype('float32')\n",
        "\n",
        "input_shape = (1, 60, 87)\n",
        "# convert class vectors to binary class matrices\n",
        "#y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "#y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print (X_train.shape, y_train.shape)\n",
        "print (X_test.shape, y_test.shape)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(30581, 1, 60, 87) (30581,)\n",
            "(10793, 1, 60, 87) (10793,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VKGfhskBZVkD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "c_w = { 0:0.02, 1:0.98}\n",
        "index = ['r%d' % x for x in range(len(y_train))]\n",
        "a = [c_w[class_label] for class_label in y_train]\n",
        "sample_weight_frame = pd.DataFrame(a, index = index)\n",
        "score_params = {\"sample_weight\": sample_weight_frame}\n",
        "#print(sample_weight_frame[0:5])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yFE3UDjTMJ9x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://github.com/keras-team/keras/issues/2115\n",
        "\n",
        "### SCORERS\n",
        "import tensorflow as tf\n",
        "import functools\n",
        "from functools import partial, update_wrapper\n",
        "\n",
        "def my_score(y_true, y_pred, sample_weight): \n",
        "  return log_loss(y_true.values, y_pred, sample_weight=sample_weight.loc[y_true.index.values].values.reshape(-1), normalize=True)\n",
        "\n",
        "def wrapped_partial(func, *args, **kwargs):\n",
        "\tpartial_func = partial(func, *args, **kwargs)\n",
        "\tupdate_wrapper(partial_func, func)\n",
        "\treturn partial_func\n",
        "\n",
        "def binary_crossentropy_weigted(y_true, y_pred, class_weights):\n",
        "\ty_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
        "\tloss = K.mean(class_weights*(-y_true * K.log(y_pred) - (1.0 - y_true) * K.log(1.0 - y_pred)),axis=-1)\n",
        "\treturn loss\n",
        "\n",
        "my_scorer = make_scorer(my_score,greater_is_better=False, needs_threshold=False,**score_params)  ## scoring for gridsearchCV\n",
        "custom_loss = wrapped_partial(binary_crossentropy_weigted, class_weights=np.array([0.02, 0.98])) ## scoring for model.compile\n",
        "\n",
        "\n",
        "## AUC METRIC\n",
        "def as_keras_metric(method):\n",
        "    import functools\n",
        "    from keras import backend as K\n",
        "    \n",
        "    @functools.wraps(method)\n",
        "    def wrapper(self, args, **kwargs):\n",
        "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
        "        value, update_op = method(self, args, **kwargs)\n",
        "        K.get_session().run(tf.local_variables_initializer())\n",
        "        with tf.control_dependencies([update_op]):\n",
        "            value = tf.identity(value)\n",
        "        return value\n",
        "    return wrapper\n",
        "  \n",
        "auc_roc = as_keras_metric(tf.metrics.auc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5VoHqo92-BG1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**BUILDING THE MODEL**"
      ]
    },
    {
      "metadata": {
        "id": "pDsoy0g8VlB5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_model_modified(dense_layer_sizes,filters, kernel_size,pool_size,lr, drop_out):\n",
        "        \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters, kernel_size,input_shape=input_shape, activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "   \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(dense_layer_sizes, activation='relu'))\n",
        "    model.add(Dropout(drop_out))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(drop_out))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    model.compile(loss=custom_loss,#'binary_crossentropy',#loss_2_args(y_true, y_pred)'',#loss='binary_crossentropy'\n",
        "                  # co tu powinno wejść? o samo co do scoring w GridSearchCv? chyba nie: https://stackoverflow.com/questions/40572743/scikit-learn-grid-search-own-scoring-object-syntax\n",
        "                  # my_score2(sample_weight = sample_weight_frame), <------------------- nie umiem zrobić by działał tutaj. Która funkcja? my_score czy my_scorer ?\n",
        "                  # my_score2(sample_weight_frame) ---->  https://stackoverflow.com/questions/46858016/keras-custom-loss-function-to-pass-arguments-other-than-y-true-and-y-pred\n",
        "                  # my_loss ---------------------------->też nie chodzi a chyba powinien, coś z formatem nie tak.  ERROR: 'Tensor' object has no attribute 'values'\n",
        "                  # inne źródło: https://github.com/keras-team/keras/issues/2115\n",
        "                  optimizer=keras.optimizers.Adam(lr),\n",
        "                  metrics=['accuracy',auc_roc])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "59IhKe5b95g-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**GRID SEARCH**\n"
      ]
    },
    {
      "metadata": {
        "id": "7YQJvxfSN1Iy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "my_classifier = KerasClassifier(make_model_modified)\n",
        "validator = GridSearchCV(my_classifier,\n",
        "                         param_grid={'dense_layer_sizes': [128],\n",
        "                                     # epochs is avail for tuning even when not\n",
        "                                     # an argument to model building function\n",
        "                                     'epochs': [80], #[10,20,30],\n",
        "                                     'filters': [10],\n",
        "                                     'kernel_size': [(3,3)],\n",
        "                                     'pool_size': [(2,2)],\n",
        "                                     # 'class_weight': [{ 0:0.02, 1:0.98}],  <------------------------------- może w ten sposób? Ale t też tylko w grid searchu\n",
        "                                     'batch_size': [32], \n",
        "                                     'drop_out':[0.5],#[0.2,0.5],\n",
        "                                     'lr': [0.001]}, #[0.01,0.001,0.0001]},\n",
        "                         scoring=my_scorer, n_jobs=1, refit=True, cv = 2)  ## SCORING ZMIENIONY na my_scorer (inne, np: 'roc_auc')\n",
        "\n",
        "##https://stackoverflow.com/questions/49581104/sklearn-gridsearchcv-not-using-sample-weight-in-score-function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mLm9knT-1OjH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "class PlotLosses(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        \n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.auc.append(logs.get('auc'))\n",
        "        self.i += 1\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        plt.plot(self.x, self.losses, label=\"loss\")\n",
        "        plt.plot(self.x, self.auc, label=\"auc\")\n",
        "        plt.legend()\n",
        "        plt.show();\n",
        "        \n",
        "plot_losses = PlotLosses()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qC9TynBWTR0E",
        "colab_type": "code",
        "outputId": "8dd914e9-c966-470e-a3a2-ba4324c5ef9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7005
        }
      },
      "cell_type": "code",
      "source": [
        "############\n",
        "from keras.callbacks import EarlyStopping\n",
        "es = EarlyStopping(monitor='auc', min_delta=0.001, patience=15, verbose=0, mode='max', baseline=None, restore_best_weights=True)\n",
        "#early_stop = EarlyStopping()\n",
        "y_frame = pd.DataFrame(y_train, index=index)\n",
        "grid_result = validator.fit(X_train, y_frame, callbacks=[es]) #, class_weight = c_w)\n",
        "\n",
        "############\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print('The parameters of the best model are: ')\n",
        "print(validator.best_params_)\n",
        "\n",
        "best_model = validator.best_estimator_.model\n",
        "metric_names = best_model.metrics_names\n",
        "metric_values = best_model.evaluate(X_test, y_test)\n",
        "for metric, value in zip(metric_names, metric_values):\n",
        "  print(metric, ': ', value)\n",
        "  \n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "  \n",
        "# calculate AUC of final model on a test set\n",
        "probs = best_model.predict_proba(X_test)\n",
        "# keep probabilities for the positive outcome only\n",
        "#probs = probs[:, 1]\n",
        "y_test2 = numpy.load('drive/My Drive/y_test.npy')  # osobno, bo inny wymiar\n",
        "\n",
        "auc = roc_auc_score(y_test2, probs)\n",
        "print('AUC: %.3f' % auc)\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test2, probs)\n",
        "# plot no skill\n",
        "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot(fpr, tpr, marker='.')\n",
        "# show the plot\n",
        "plt.title('ROC curve for test set')\n",
        "pyplot.show()\n",
        "\n",
        "with open('drive/My Drive/ES.txt', 'w') as f:\n",
        "  print('Filename:', validator.best_params_, file=f)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "15290/15290 [==============================] - 11s 702us/step - loss: 0.0489 - acc: 0.9881 - auc: 0.4805\n",
            "Epoch 2/80\n",
            "15290/15290 [==============================] - 9s 621us/step - loss: 0.0377 - acc: 0.9885 - auc: 0.4898\n",
            "Epoch 3/80\n",
            "15290/15290 [==============================] - 10s 628us/step - loss: 0.0357 - acc: 0.9885 - auc: 0.4985\n",
            "Epoch 4/80\n",
            "15290/15290 [==============================] - 10s 626us/step - loss: 0.0372 - acc: 0.9885 - auc: 0.5046\n",
            "Epoch 5/80\n",
            "15290/15290 [==============================] - 9s 609us/step - loss: 0.0366 - acc: 0.9885 - auc: 0.5000\n",
            "Epoch 6/80\n",
            "15290/15290 [==============================] - 10s 624us/step - loss: 0.0352 - acc: 0.9885 - auc: 0.5012\n",
            "Epoch 7/80\n",
            "15290/15290 [==============================] - 10s 623us/step - loss: 0.0356 - acc: 0.9885 - auc: 0.5045\n",
            "Epoch 8/80\n",
            "15290/15290 [==============================] - 9s 617us/step - loss: 0.0348 - acc: 0.9885 - auc: 0.5065\n",
            "Epoch 9/80\n",
            "15290/15290 [==============================] - 9s 608us/step - loss: 0.0345 - acc: 0.9885 - auc: 0.5081\n",
            "Epoch 10/80\n",
            "15290/15290 [==============================] - 9s 611us/step - loss: 0.0355 - acc: 0.9885 - auc: 0.5052\n",
            "Epoch 11/80\n",
            "15290/15290 [==============================] - 9s 619us/step - loss: 0.0348 - acc: 0.9885 - auc: 0.5032\n",
            "Epoch 12/80\n",
            "15290/15290 [==============================] - 9s 615us/step - loss: 0.0338 - acc: 0.9885 - auc: 0.5021\n",
            "Epoch 13/80\n",
            "15290/15290 [==============================] - 9s 618us/step - loss: 0.0333 - acc: 0.9885 - auc: 0.5025\n",
            "Epoch 14/80\n",
            "15290/15290 [==============================] - 9s 613us/step - loss: 0.0338 - acc: 0.9885 - auc: 0.5031\n",
            "Epoch 15/80\n",
            "15290/15290 [==============================] - 9s 616us/step - loss: 0.0335 - acc: 0.9885 - auc: 0.5041\n",
            "Epoch 16/80\n",
            "15290/15290 [==============================] - 9s 614us/step - loss: 0.0330 - acc: 0.9885 - auc: 0.5045\n",
            "Epoch 17/80\n",
            "15290/15290 [==============================] - 9s 618us/step - loss: 0.0331 - acc: 0.9886 - auc: 0.5056\n",
            "Epoch 18/80\n",
            "15290/15290 [==============================] - 9s 610us/step - loss: 0.0329 - acc: 0.9887 - auc: 0.5063\n",
            "Epoch 19/80\n",
            "15290/15290 [==============================] - 9s 617us/step - loss: 0.0333 - acc: 0.9886 - auc: 0.5076\n",
            "Epoch 20/80\n",
            "15290/15290 [==============================] - 9s 618us/step - loss: 0.0332 - acc: 0.9886 - auc: 0.5068\n",
            "Epoch 21/80\n",
            "15290/15290 [==============================] - 10s 624us/step - loss: 0.0478 - acc: 0.9888 - auc: 0.5074\n",
            "Epoch 22/80\n",
            "15290/15290 [==============================] - 9s 617us/step - loss: 0.0922 - acc: 0.9886 - auc: 0.5081\n",
            "Epoch 23/80\n",
            "15290/15290 [==============================] - 9s 620us/step - loss: 0.0599 - acc: 0.9885 - auc: 0.5080\n",
            "Epoch 24/80\n",
            "15290/15290 [==============================] - 9s 620us/step - loss: 0.0322 - acc: 0.9886 - auc: 0.5086\n",
            "Epoch 1/80\n",
            "15291/15291 [==============================] - 10s 656us/step - loss: 0.0750 - acc: 0.9738 - auc: 0.5174\n",
            "Epoch 2/80\n",
            "15291/15291 [==============================] - 10s 623us/step - loss: 0.0643 - acc: 0.9750 - auc: 0.5425\n",
            "Epoch 3/80\n",
            "15291/15291 [==============================] - 9s 612us/step - loss: 0.0639 - acc: 0.9750 - auc: 0.5562\n",
            "Epoch 4/80\n",
            "15291/15291 [==============================] - 9s 614us/step - loss: 0.0620 - acc: 0.9750 - auc: 0.5598\n",
            "Epoch 5/80\n",
            "15291/15291 [==============================] - 9s 617us/step - loss: 0.0622 - acc: 0.9750 - auc: 0.5644\n",
            "Epoch 6/80\n",
            "15291/15291 [==============================] - 9s 617us/step - loss: 0.0617 - acc: 0.9750 - auc: 0.5674\n",
            "Epoch 7/80\n",
            "15291/15291 [==============================] - 9s 607us/step - loss: 0.0620 - acc: 0.9750 - auc: 0.5690\n",
            "Epoch 8/80\n",
            "15291/15291 [==============================] - 9s 614us/step - loss: 0.0616 - acc: 0.9750 - auc: 0.5690\n",
            "Epoch 9/80\n",
            "15291/15291 [==============================] - 9s 618us/step - loss: 0.0613 - acc: 0.9750 - auc: 0.5710\n",
            "Epoch 10/80\n",
            "15291/15291 [==============================] - 9s 613us/step - loss: 0.0608 - acc: 0.9750 - auc: 0.5708\n",
            "Epoch 11/80\n",
            "15291/15291 [==============================] - 9s 613us/step - loss: 0.0599 - acc: 0.9750 - auc: 0.5715\n",
            "Epoch 12/80\n",
            "15291/15291 [==============================] - 10s 625us/step - loss: 0.0593 - acc: 0.9750 - auc: 0.5721\n",
            "Epoch 13/80\n",
            "15291/15291 [==============================] - 9s 617us/step - loss: 0.0587 - acc: 0.9750 - auc: 0.5736\n",
            "Epoch 14/80\n",
            "15291/15291 [==============================] - 9s 615us/step - loss: 0.0587 - acc: 0.9750 - auc: 0.5756\n",
            "Epoch 15/80\n",
            "15291/15291 [==============================] - 9s 610us/step - loss: 0.0578 - acc: 0.9750 - auc: 0.5767\n",
            "Epoch 16/80\n",
            "15291/15291 [==============================] - 10s 621us/step - loss: 0.0582 - acc: 0.9750 - auc: 0.5788\n",
            "Epoch 17/80\n",
            "15291/15291 [==============================] - 9s 609us/step - loss: 0.0573 - acc: 0.9752 - auc: 0.5809\n",
            "Epoch 18/80\n",
            "15291/15291 [==============================] - 9s 608us/step - loss: 0.0549 - acc: 0.9760 - auc: 0.5854\n",
            "Epoch 19/80\n",
            "15291/15291 [==============================] - 9s 620us/step - loss: 0.0523 - acc: 0.9770 - auc: 0.5908\n",
            "Epoch 20/80\n",
            "15291/15291 [==============================] - 9s 616us/step - loss: 0.0494 - acc: 0.9783 - auc: 0.5985\n",
            "Epoch 21/80\n",
            "15291/15291 [==============================] - 9s 613us/step - loss: 0.0469 - acc: 0.9792 - auc: 0.6058\n",
            "Epoch 22/80\n",
            "15291/15291 [==============================] - 9s 608us/step - loss: 0.0435 - acc: 0.9802 - auc: 0.6159\n",
            "Epoch 23/80\n",
            "15291/15291 [==============================] - 9s 615us/step - loss: 0.0417 - acc: 0.9812 - auc: 0.6262\n",
            "Epoch 24/80\n",
            "15291/15291 [==============================] - 10s 622us/step - loss: 0.0412 - acc: 0.9819 - auc: 0.6371\n",
            "Epoch 25/80\n",
            "15291/15291 [==============================] - 10s 625us/step - loss: 0.0389 - acc: 0.9838 - auc: 0.6476\n",
            "Epoch 26/80\n",
            "15291/15291 [==============================] - 9s 618us/step - loss: 0.0359 - acc: 0.9848 - auc: 0.6585\n",
            "Epoch 27/80\n",
            "15291/15291 [==============================] - 9s 610us/step - loss: 0.0344 - acc: 0.9852 - auc: 0.6682\n",
            "Epoch 28/80\n",
            "15291/15291 [==============================] - 9s 617us/step - loss: 0.0350 - acc: 0.9852 - auc: 0.6780\n",
            "Epoch 29/80\n",
            "15291/15291 [==============================] - 9s 621us/step - loss: 0.0332 - acc: 0.9862 - auc: 0.6870\n",
            "Epoch 30/80\n",
            "15291/15291 [==============================] - 9s 617us/step - loss: 0.0310 - acc: 0.9869 - auc: 0.6958\n",
            "Epoch 31/80\n",
            "15291/15291 [==============================] - 9s 621us/step - loss: 0.0301 - acc: 0.9867 - auc: 0.7042\n",
            "Epoch 32/80\n",
            "15291/15291 [==============================] - 9s 611us/step - loss: 0.0382 - acc: 0.9858 - auc: 0.7118\n",
            "Epoch 33/80\n",
            "15291/15291 [==============================] - 9s 609us/step - loss: 0.0315 - acc: 0.9863 - auc: 0.7180\n",
            "Epoch 34/80\n",
            "15291/15291 [==============================] - 9s 603us/step - loss: 0.0297 - acc: 0.9876 - auc: 0.7249\n",
            "Epoch 35/80\n",
            "15291/15291 [==============================] - 9s 601us/step - loss: 0.0312 - acc: 0.9859 - auc: 0.7310\n",
            "Epoch 36/80\n",
            "15291/15291 [==============================] - 9s 614us/step - loss: 0.0299 - acc: 0.9874 - auc: 0.7372\n",
            "Epoch 37/80\n",
            "15291/15291 [==============================] - 9s 614us/step - loss: 0.0274 - acc: 0.9885 - auc: 0.7434\n",
            "Epoch 38/80\n",
            "15291/15291 [==============================] - 10s 628us/step - loss: 0.0280 - acc: 0.9880 - auc: 0.7491\n",
            "Epoch 39/80\n",
            "15291/15291 [==============================] - 9s 613us/step - loss: 0.0269 - acc: 0.9887 - auc: 0.7548\n",
            "Epoch 40/80\n",
            "15291/15291 [==============================] - 9s 616us/step - loss: 0.0263 - acc: 0.9888 - auc: 0.7602\n",
            "Epoch 41/80\n",
            "15291/15291 [==============================] - 9s 613us/step - loss: 0.0261 - acc: 0.9890 - auc: 0.7652\n",
            "Epoch 42/80\n",
            "15291/15291 [==============================] - 9s 620us/step - loss: 0.0272 - acc: 0.9887 - auc: 0.7701\n",
            "Epoch 43/80\n",
            "15291/15291 [==============================] - 9s 613us/step - loss: 0.0255 - acc: 0.9893 - auc: 0.7749\n",
            "Epoch 44/80\n",
            "15291/15291 [==============================] - 9s 612us/step - loss: 0.0310 - acc: 0.9873 - auc: 0.7792\n",
            "Epoch 45/80\n",
            "15291/15291 [==============================] - 9s 609us/step - loss: 0.0285 - acc: 0.9890 - auc: 0.7827\n",
            "Epoch 46/80\n",
            "15291/15291 [==============================] - 9s 604us/step - loss: 0.0276 - acc: 0.9880 - auc: 0.7868\n",
            "Epoch 47/80\n",
            "15291/15291 [==============================] - 9s 614us/step - loss: 0.0230 - acc: 0.9893 - auc: 0.7904\n",
            "Epoch 48/80\n",
            "15291/15291 [==============================] - 9s 617us/step - loss: 0.0242 - acc: 0.9891 - auc: 0.7944\n",
            "Epoch 49/80\n",
            "15291/15291 [==============================] - 9s 616us/step - loss: 0.0283 - acc: 0.9872 - auc: 0.7981\n",
            "Epoch 50/80\n",
            "15291/15291 [==============================] - 9s 618us/step - loss: 0.0247 - acc: 0.9888 - auc: 0.8013\n",
            "Epoch 51/80\n",
            "15291/15291 [==============================] - 9s 618us/step - loss: 0.0242 - acc: 0.9894 - auc: 0.8043\n",
            "Epoch 52/80\n",
            "15291/15291 [==============================] - 9s 616us/step - loss: 0.0222 - acc: 0.9903 - auc: 0.8078\n",
            "Epoch 53/80\n",
            "15291/15291 [==============================] - 9s 621us/step - loss: 0.0225 - acc: 0.9903 - auc: 0.8113\n",
            "Epoch 54/80\n",
            "15291/15291 [==============================] - 9s 613us/step - loss: 0.0218 - acc: 0.9903 - auc: 0.8146\n",
            "Epoch 55/80\n",
            "15291/15291 [==============================] - 9s 616us/step - loss: 0.0197 - acc: 0.9911 - auc: 0.8179\n",
            "Epoch 56/80\n",
            "15291/15291 [==============================] - 9s 618us/step - loss: 0.0235 - acc: 0.9897 - auc: 0.8206\n",
            "Epoch 57/80\n",
            "15291/15291 [==============================] - 9s 618us/step - loss: 0.0233 - acc: 0.9897 - auc: 0.8235\n",
            "Epoch 58/80\n",
            "15291/15291 [==============================] - 10s 630us/step - loss: 0.0201 - acc: 0.9906 - auc: 0.8264\n",
            "Epoch 59/80\n",
            "15291/15291 [==============================] - 9s 617us/step - loss: 0.0198 - acc: 0.9909 - auc: 0.8293\n",
            "Epoch 60/80\n",
            "15291/15291 [==============================] - 9s 613us/step - loss: 0.0218 - acc: 0.9900 - auc: 0.8318\n",
            "Epoch 61/80\n",
            "15291/15291 [==============================] - 9s 615us/step - loss: 0.0224 - acc: 0.9902 - auc: 0.8344\n",
            "Epoch 62/80\n",
            "15291/15291 [==============================] - 9s 621us/step - loss: 0.0173 - acc: 0.9921 - auc: 0.8370\n",
            "Epoch 63/80\n",
            "15291/15291 [==============================] - 9s 620us/step - loss: 0.0169 - acc: 0.9919 - auc: 0.8399\n",
            "Epoch 64/80\n",
            "15291/15291 [==============================] - 9s 618us/step - loss: 0.0217 - acc: 0.9901 - auc: 0.8424\n",
            "Epoch 65/80\n",
            "15291/15291 [==============================] - 9s 611us/step - loss: 0.0199 - acc: 0.9918 - auc: 0.8447\n",
            "Epoch 66/80\n",
            "15291/15291 [==============================] - 9s 619us/step - loss: 0.0181 - acc: 0.9918 - auc: 0.8470\n",
            "Epoch 67/80\n",
            "15291/15291 [==============================] - 10s 623us/step - loss: 0.0182 - acc: 0.9916 - auc: 0.8493\n",
            "Epoch 68/80\n",
            "15291/15291 [==============================] - 10s 623us/step - loss: 0.0271 - acc: 0.9888 - auc: 0.8510\n",
            "Epoch 69/80\n",
            "15291/15291 [==============================] - 9s 620us/step - loss: 0.0271 - acc: 0.9884 - auc: 0.8522\n",
            "Epoch 70/80\n",
            "15291/15291 [==============================] - 9s 618us/step - loss: 0.0200 - acc: 0.9906 - auc: 0.8537\n",
            "Epoch 71/80\n",
            "15291/15291 [==============================] - 9s 620us/step - loss: 0.0181 - acc: 0.9921 - auc: 0.8557\n",
            "Epoch 72/80\n",
            "15291/15291 [==============================] - 9s 616us/step - loss: 0.0198 - acc: 0.9914 - auc: 0.8576\n",
            "Epoch 73/80\n",
            "15291/15291 [==============================] - 9s 621us/step - loss: 0.0190 - acc: 0.9916 - auc: 0.8593\n",
            "Epoch 74/80\n",
            "15291/15291 [==============================] - 9s 615us/step - loss: 0.0167 - acc: 0.9924 - auc: 0.8611\n",
            "Epoch 75/80\n",
            "15291/15291 [==============================] - 10s 630us/step - loss: 0.0159 - acc: 0.9930 - auc: 0.8632\n",
            "Epoch 76/80\n",
            "15291/15291 [==============================] - 10s 628us/step - loss: 0.0169 - acc: 0.9919 - auc: 0.8649\n",
            "Epoch 77/80\n",
            "15291/15291 [==============================] - 9s 615us/step - loss: 0.0169 - acc: 0.9924 - auc: 0.8667\n",
            "Epoch 78/80\n",
            "15291/15291 [==============================] - 10s 623us/step - loss: 0.0181 - acc: 0.9918 - auc: 0.8684\n",
            "Epoch 79/80\n",
            "15291/15291 [==============================] - 10s 628us/step - loss: 0.0140 - acc: 0.9935 - auc: 0.8699\n",
            "Epoch 80/80\n",
            "15291/15291 [==============================] - 10s 638us/step - loss: 0.0163 - acc: 0.9923 - auc: 0.8717\n",
            "Epoch 1/80\n",
            "30581/30581 [==============================] - 14s 468us/step - loss: 0.0578 - acc: 0.9815 - auc: 0.5262\n",
            "Epoch 2/80\n",
            "30581/30581 [==============================] - 14s 444us/step - loss: 0.0512 - acc: 0.9817 - auc: 0.5264\n",
            "Epoch 3/80\n",
            "30581/30581 [==============================] - 13s 438us/step - loss: 0.0501 - acc: 0.9817 - auc: 0.5294\n",
            "Epoch 4/80\n",
            "30581/30581 [==============================] - 13s 436us/step - loss: 0.0496 - acc: 0.9817 - auc: 0.5322\n",
            "Epoch 5/80\n",
            "30581/30581 [==============================] - 13s 432us/step - loss: 0.0487 - acc: 0.9817 - auc: 0.5329\n",
            "Epoch 6/80\n",
            "30581/30581 [==============================] - 13s 418us/step - loss: 0.0482 - acc: 0.9817 - auc: 0.5340\n",
            "Epoch 7/80\n",
            "30581/30581 [==============================] - 12s 392us/step - loss: 0.0471 - acc: 0.9817 - auc: 0.5377\n",
            "Epoch 8/80\n",
            "30581/30581 [==============================] - 12s 394us/step - loss: 0.0473 - acc: 0.9817 - auc: 0.5404\n",
            "Epoch 9/80\n",
            "30581/30581 [==============================] - 12s 388us/step - loss: 0.0479 - acc: 0.9817 - auc: 0.5391\n",
            "Epoch 10/80\n",
            "30581/30581 [==============================] - 12s 388us/step - loss: 0.0472 - acc: 0.9817 - auc: 0.5385\n",
            "Epoch 11/80\n",
            "30581/30581 [==============================] - 12s 389us/step - loss: 0.0472 - acc: 0.9817 - auc: 0.5382\n",
            "Epoch 12/80\n",
            "30581/30581 [==============================] - 12s 387us/step - loss: 0.0464 - acc: 0.9817 - auc: 0.5386\n",
            "Epoch 13/80\n",
            "30581/30581 [==============================] - 12s 388us/step - loss: 0.0456 - acc: 0.9819 - auc: 0.5415\n",
            "Epoch 14/80\n",
            "30581/30581 [==============================] - 12s 388us/step - loss: 0.0437 - acc: 0.9824 - auc: 0.5468\n",
            "Epoch 15/80\n",
            "30581/30581 [==============================] - 12s 389us/step - loss: 0.0415 - acc: 0.9830 - auc: 0.5552\n",
            "Epoch 16/80\n",
            "30581/30581 [==============================] - 12s 388us/step - loss: 0.0374 - acc: 0.9848 - auc: 0.5685\n",
            "Epoch 17/80\n",
            "30581/30581 [==============================] - 12s 387us/step - loss: 0.0357 - acc: 0.9857 - auc: 0.5831\n",
            "Epoch 18/80\n",
            "30581/30581 [==============================] - 12s 387us/step - loss: 0.0335 - acc: 0.9868 - auc: 0.5978\n",
            "Epoch 19/80\n",
            "30581/30581 [==============================] - 12s 389us/step - loss: 0.0323 - acc: 0.9874 - auc: 0.6120\n",
            "Epoch 20/80\n",
            "30581/30581 [==============================] - 12s 388us/step - loss: 0.0338 - acc: 0.9868 - auc: 0.6251\n",
            "Epoch 21/80\n",
            "30581/30581 [==============================] - 12s 389us/step - loss: 0.0316 - acc: 0.9868 - auc: 0.6367\n",
            "Epoch 22/80\n",
            "30581/30581 [==============================] - 12s 387us/step - loss: 0.0310 - acc: 0.9873 - auc: 0.6467\n",
            "Epoch 23/80\n",
            "30581/30581 [==============================] - 12s 389us/step - loss: 0.0306 - acc: 0.9879 - auc: 0.6567\n",
            "Epoch 24/80\n",
            "30581/30581 [==============================] - 12s 387us/step - loss: 0.0291 - acc: 0.9883 - auc: 0.6662\n",
            "Epoch 25/80\n",
            "30581/30581 [==============================] - 12s 388us/step - loss: 0.0292 - acc: 0.9881 - auc: 0.6760\n",
            "Epoch 26/80\n",
            "30581/30581 [==============================] - 12s 388us/step - loss: 0.0283 - acc: 0.9888 - auc: 0.6840\n",
            "Epoch 27/80\n",
            "30581/30581 [==============================] - 12s 387us/step - loss: 0.0293 - acc: 0.9886 - auc: 0.6911\n",
            "Epoch 28/80\n",
            "30581/30581 [==============================] - 12s 391us/step - loss: 0.0270 - acc: 0.9893 - auc: 0.6979\n",
            "Epoch 29/80\n",
            "30581/30581 [==============================] - 12s 387us/step - loss: 0.0280 - acc: 0.9885 - auc: 0.7046\n",
            "Epoch 30/80\n",
            "30581/30581 [==============================] - 12s 388us/step - loss: 0.0267 - acc: 0.9891 - auc: 0.7106\n",
            "Epoch 31/80\n",
            "30581/30581 [==============================] - 12s 388us/step - loss: 0.0264 - acc: 0.9896 - auc: 0.7166\n",
            "Epoch 32/80\n",
            "30581/30581 [==============================] - 12s 391us/step - loss: 0.0253 - acc: 0.9901 - auc: 0.7229\n",
            "Epoch 33/80\n",
            "30581/30581 [==============================] - 12s 388us/step - loss: 0.0254 - acc: 0.9897 - auc: 0.7284\n",
            "Epoch 34/80\n",
            "30581/30581 [==============================] - 12s 388us/step - loss: 0.0249 - acc: 0.9902 - auc: 0.7339\n",
            "Epoch 35/80\n",
            "30581/30581 [==============================] - 12s 386us/step - loss: 0.0248 - acc: 0.9902 - auc: 0.7397\n",
            "Epoch 36/80\n",
            "30581/30581 [==============================] - 12s 390us/step - loss: 0.0252 - acc: 0.9899 - auc: 0.7438\n",
            "Epoch 37/80\n",
            "30581/30581 [==============================] - 12s 387us/step - loss: 0.0253 - acc: 0.9901 - auc: 0.7488\n",
            "Epoch 38/80\n",
            "30581/30581 [==============================] - 12s 387us/step - loss: 0.0224 - acc: 0.9903 - auc: 0.7534\n",
            "Epoch 39/80\n",
            "30581/30581 [==============================] - 12s 390us/step - loss: 0.0228 - acc: 0.9908 - auc: 0.7583\n",
            "Epoch 40/80\n",
            "30581/30581 [==============================] - 12s 386us/step - loss: 0.0234 - acc: 0.9906 - auc: 0.7628\n",
            "Epoch 41/80\n",
            "30581/30581 [==============================] - 12s 386us/step - loss: 0.0224 - acc: 0.9907 - auc: 0.7672\n",
            "Epoch 42/80\n",
            "30581/30581 [==============================] - 12s 387us/step - loss: 0.0239 - acc: 0.9904 - auc: 0.7717\n",
            "Epoch 43/80\n",
            "30581/30581 [==============================] - 12s 386us/step - loss: 0.0216 - acc: 0.9911 - auc: 0.7760\n",
            "Epoch 44/80\n",
            "30581/30581 [==============================] - 12s 386us/step - loss: 0.0217 - acc: 0.9913 - auc: 0.7799\n",
            "Epoch 45/80\n",
            "30581/30581 [==============================] - 12s 385us/step - loss: 0.0210 - acc: 0.9909 - auc: 0.7838\n",
            "Epoch 46/80\n",
            "30581/30581 [==============================] - 12s 386us/step - loss: 0.0210 - acc: 0.9914 - auc: 0.7873\n",
            "Epoch 47/80\n",
            "30581/30581 [==============================] - 12s 384us/step - loss: 0.0196 - acc: 0.9921 - auc: 0.7907\n",
            "Epoch 48/80\n",
            "30581/30581 [==============================] - 12s 386us/step - loss: 0.0191 - acc: 0.9917 - auc: 0.7945\n",
            "Epoch 49/80\n",
            "30581/30581 [==============================] - 12s 387us/step - loss: 0.0193 - acc: 0.9920 - auc: 0.7981\n",
            "Epoch 50/80\n",
            "30581/30581 [==============================] - 12s 385us/step - loss: 0.0193 - acc: 0.9918 - auc: 0.8015\n",
            "Epoch 51/80\n",
            "30581/30581 [==============================] - 12s 385us/step - loss: 0.0194 - acc: 0.9915 - auc: 0.8046\n",
            "Epoch 52/80\n",
            "30581/30581 [==============================] - 12s 386us/step - loss: 0.0189 - acc: 0.9922 - auc: 0.8078\n",
            "Epoch 53/80\n",
            "30581/30581 [==============================] - 12s 386us/step - loss: 0.0188 - acc: 0.9923 - auc: 0.8107\n",
            "Epoch 54/80\n",
            "30581/30581 [==============================] - 12s 386us/step - loss: 0.0194 - acc: 0.9915 - auc: 0.8134\n",
            "Epoch 55/80\n",
            "30581/30581 [==============================] - 12s 385us/step - loss: 0.0192 - acc: 0.9917 - auc: 0.8163\n",
            "Epoch 56/80\n",
            "30581/30581 [==============================] - 12s 385us/step - loss: 0.0189 - acc: 0.9921 - auc: 0.8184\n",
            "Epoch 57/80\n",
            "30581/30581 [==============================] - 12s 387us/step - loss: 0.0180 - acc: 0.9924 - auc: 0.8211\n",
            "Epoch 58/80\n",
            "30581/30581 [==============================] - 12s 388us/step - loss: 0.0176 - acc: 0.9922 - auc: 0.8236\n",
            "Epoch 59/80\n",
            "30581/30581 [==============================] - 12s 390us/step - loss: 0.0188 - acc: 0.9923 - auc: 0.8259\n",
            "Epoch 60/80\n",
            "30581/30581 [==============================] - 12s 384us/step - loss: 0.0175 - acc: 0.9922 - auc: 0.8282\n",
            "Epoch 61/80\n",
            "30581/30581 [==============================] - 12s 384us/step - loss: 0.0177 - acc: 0.9927 - auc: 0.8304\n",
            "Epoch 62/80\n",
            "30581/30581 [==============================] - 12s 390us/step - loss: 0.0164 - acc: 0.9930 - auc: 0.8326\n",
            "Epoch 63/80\n",
            "30581/30581 [==============================] - 12s 386us/step - loss: 0.0170 - acc: 0.9931 - auc: 0.8348\n",
            "Epoch 64/80\n",
            "30581/30581 [==============================] - 12s 386us/step - loss: 0.0168 - acc: 0.9925 - auc: 0.8370\n",
            "Epoch 65/80\n",
            "30581/30581 [==============================] - 12s 387us/step - loss: 0.0186 - acc: 0.9926 - auc: 0.8389\n",
            "Epoch 66/80\n",
            "30581/30581 [==============================] - 12s 386us/step - loss: 0.0160 - acc: 0.9934 - auc: 0.8408\n",
            "Epoch 67/80\n",
            "30581/30581 [==============================] - 12s 386us/step - loss: 0.0159 - acc: 0.9931 - auc: 0.8429\n",
            "Epoch 68/80\n",
            "30581/30581 [==============================] - 12s 386us/step - loss: 0.0172 - acc: 0.9930 - auc: 0.8448\n",
            "Epoch 69/80\n",
            "30581/30581 [==============================] - 12s 391us/step - loss: 0.0164 - acc: 0.9928 - auc: 0.8466\n",
            "Epoch 70/80\n",
            "30581/30581 [==============================] - 12s 395us/step - loss: 0.0157 - acc: 0.9933 - auc: 0.8484\n",
            "Epoch 71/80\n",
            "30581/30581 [==============================] - 12s 391us/step - loss: 0.0159 - acc: 0.9932 - auc: 0.8502\n",
            "Epoch 72/80\n",
            "30581/30581 [==============================] - 12s 387us/step - loss: 0.0154 - acc: 0.9933 - auc: 0.8520\n",
            "Epoch 73/80\n",
            "30581/30581 [==============================] - 12s 389us/step - loss: 0.0145 - acc: 0.9937 - auc: 0.8536\n",
            "Epoch 74/80\n",
            "30581/30581 [==============================] - 12s 387us/step - loss: 0.0172 - acc: 0.9932 - auc: 0.8552\n",
            "Epoch 75/80\n",
            "30581/30581 [==============================] - 12s 385us/step - loss: 0.0149 - acc: 0.9935 - auc: 0.8568\n",
            "Epoch 76/80\n",
            "30581/30581 [==============================] - 12s 386us/step - loss: 0.0135 - acc: 0.9946 - auc: 0.8583\n",
            "Epoch 77/80\n",
            "30581/30581 [==============================] - 12s 388us/step - loss: 0.0140 - acc: 0.9942 - auc: 0.8598\n",
            "Epoch 78/80\n",
            "30581/30581 [==============================] - 12s 387us/step - loss: 0.0149 - acc: 0.9939 - auc: 0.8614\n",
            "Epoch 79/80\n",
            "30581/30581 [==============================] - 12s 396us/step - loss: 0.0150 - acc: 0.9937 - auc: 0.8629\n",
            "Epoch 80/80\n",
            "30581/30581 [==============================] - 12s 394us/step - loss: 0.0146 - acc: 0.9939 - auc: 0.8644\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e1e92b44572c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Plot training & validation accuracy values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "7Q9Mkpfv-jpk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pyplot.plot(y_test2[10100:10300], marker='.')\n",
        "pyplot.plot(probs[10100:10300], marker='.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f5WQIpjqJ7JV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "outputId": "682affc0-65a8-4523-de47-c1f49bff9543"
      },
      "cell_type": "code",
      "source": [
        "print('The parameters of the best model are: ')\n",
        "print(validator.best_params_)\n",
        "\n",
        "best_model = validator.best_estimator_.model\n",
        "metric_names = best_model.metrics_names\n",
        "metric_values = best_model.evaluate(X_test, y_test)\n",
        "for metric, value in zip(metric_names, metric_values):\n",
        "  print(metric, ': ', value)\n",
        "  \n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "  \n",
        "# calculate AUC of final model on a test set\n",
        "probs = best_model.predict_proba(X_test)\n",
        "# keep probabilities for the positive outcome only\n",
        "#probs = probs[:, 1]\n",
        "y_test2 = numpy.load('drive/My Drive/y_test.npy')  # osobno, bo inny wymiar\n",
        "\n",
        "auc = roc_auc_score(y_test2, probs)\n",
        "print('AUC: %.3f' % auc)\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test2, probs)\n",
        "# plot no skill\n",
        "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot(fpr, tpr, marker='.')\n",
        "# show the plot\n",
        "plt.title('ROC curve for test set')\n",
        "pyplot.show()\n",
        "\n",
        "with open('drive/My Drive/ES.txt', 'w') as f:\n",
        "  print('Filename:', validator.best_params_, file=f)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameters of the best model are: \n",
            "{'batch_size': 32, 'dense_layer_sizes': 128, 'drop_out': 0.5, 'epochs': 80, 'filters': 10, 'kernel_size': (3, 3), 'lr': 0.001, 'pool_size': (2, 2)}\n",
            "10793/10793 [==============================] - 2s 172us/step\n",
            "loss :  0.16002900443409857\n",
            "acc :  0.9595107940331696\n",
            "auc :  0.8648376393989715\n",
            "-13.325904 (5.922670) with: {'batch_size': 32, 'dense_layer_sizes': 128, 'drop_out': 0.5, 'epochs': 80, 'filters': 10, 'kernel_size': (3, 3), 'lr': 0.001, 'pool_size': (2, 2)}\n",
            "AUC: 0.792\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFZCAYAAACv05cWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmKhu6maCAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "CdQr0WX6J7qp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}